<!doctype html>
<html lang="es">
<head>
<link rel="stylesheet" type="text/css" href="base.css" />
<link rel="stylesheet" type="text/css" href="content.css" />
<link rel="stylesheet" type="text/css" href="nav.css" />
<meta http-equiv="content-type" content="text/html;  charset=utf-8" />
<title>1.5.1. Procesadores con Capacidades de IA | Tema 1 FH </title>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<meta name="author" content="Diana de Lara del Rey" />
<link rel="license" type="text/html" href="http://creativecommons.org/licenses/by-sa/4.0/" />
<meta name="generator" content="eXeLearning 2.8.1 - exelearning.net" />
<!--[if lt IE 9]><script type="text/javascript" src="exe_html5.js"></script><![endif]-->
<script type="text/javascript" src="exe_jquery.js"></script>
<script type="text/javascript" src="common_i18n.js"></script>
<script type="text/javascript" src="common.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="exe-web-site" id="exe-node-17"><script type="text/javascript">document.body.className+=" js"</script>
<div id="content">
<p id="skipNav"><a href="#main" class="sr-av">Saltar la navegación</a></p>
<header id="header" ><div id="headerContent">Tema 1 FH</div></header>
<nav id="siteNav">
<ul>
   <li><a href="index.html" class="daddy main-node">Tema 1. Arquitectura de ordenadores. Estructura funcional</a></li>
   <li><a href="11_introduccin.html" class="no-ch">1.1. Introducción</a></li>
   <li><a href="12_arquitectura_de_un_ordenador_elementos_funcionales_y_subsistemas.html" class="daddy">1.2. Arquitectura de un ordenador. Elementos funcionales y subsistemas</a>
   <ul class="other-section">
      <li><a href="121_la_memoria.html" class="no-ch">1.2.1. La memoria</a></li>
      <li><a href="122_subsistema_de_entradasalida.html" class="no-ch">1.2.2. Subsistema de entrada/salida</a></li>
      <li><a href="123_la_unidad_aritmticolgica_alu.html" class="no-ch">1.2.3. La Unidad Aritmético-Lógica (ALU)</a></li>
      <li><a href="124_la_unidad_de_control.html" class="no-ch">1.2.4. La Unidad de Control</a></li>
   </ul>
   </li>
   <li><a href="13_los_soc_procesadores_mviles.html" class="daddy">1.3. Los SoC. Procesadores móviles</a>
   <ul class="other-section">
      <li><a href="131_procesadores_usados_en_los_soc.html" class="no-ch">1.3.1. Procesadores usados en los SoC</a></li>
      <li><a href="132_procesadores_arm.html" class="daddy">1.3.2. Procesadores ARM</a>
      <ul class="other-section">
         <li><a href="1321_evolucin_de_los_procesadores_arm.html" class="no-ch">1.3.2.1. Evolución de los Procesadores ARM</a></li>
         <li><a href="1322_aplicaciones_de_los_procesadores_arm.html" class="no-ch">1.3.2.2. Aplicaciones de los Procesadores ARM</a></li>
         <li><a href="1323_ventajas_y_desventajas_de_los_procesadores_arm.html" class="no-ch">1.3.2.3. Ventajas y Desventajas de los Procesadores ARM</a></li>
         <li><a href="1324_futuro_de_arm_en_la_computacin.html" class="no-ch">1.3.2.4. Futuro de ARM en la Computación</a></li>
      </ul>
      </li>
   </ul>
   </li>
   <li><a href="14_nuevas_arquitecturas_de_los_microprocesadores_chips_neuromrficos.html" class="daddy">1.4. Nuevas arquitecturas de los microprocesadores: chips neuromórficos.</a>
   <ul class="other-section">
      <li><a href="141_arquitectura_y_funcionalidad.html" class="no-ch">1.4.1. Arquitectura y Funcionalidad</a></li>
   </ul>
   </li>
   <li class="current-page-parent"><a href="15_inteligencia_artificial_en_los_procesadores.html" class="current-page-parent daddy">1.5. Inteligencia artificial en los procesadores</a>
   <ul>
      <li id="active"><a href="151_procesadores_con_capacidades_de_ia.html" class="active daddy">1.5.1. Procesadores con Capacidades de IA</a>
      <ul>
         <li><a href="1511_diferencias_clave_entre_npu_y_tpu.html" class="no-ch">1.5.1.1. Diferencias clave entre NPU y TPU</a></li>
      </ul>
      </li>
      <li><a href="152_implementaciones_de_ia_en_los_procesadores.html" class="no-ch">1.5.2. Implementaciones de IA en los Procesadores</a></li>
      <li><a href="153_tendencias_futuras_en_inteligencia_artificial_en_procesadores.html" class="no-ch">1.5.3. Tendencias Futuras en Inteligencia Artificial en Procesadores</a></li>
   </ul>
   </li>
</ul>
</nav>
<div id='topPagination'>
<nav class="pagination noprt">
<a href="15_inteligencia_artificial_en_los_procesadores.html" class="prev"><span><span>&laquo; </span>Anterior</span></a> <span class="sep">| </span><a href="1511_diferencias_clave_entre_npu_y_tpu.html" class="next"><span>Siguiente<span> &raquo;</span></span></a>
</nav>
</div>
<div id="main-wrapper">
<section id="main">
<header id="nodeDecoration"><h1 id="nodeTitle">1.5.1. Procesadores con Capacidades de IA</h1></header>
<article class="iDevice_wrapper textIdevice" id="id58">
<div class="iDevice emphasis0" >
<div id="ta58_154_2" class="block iDevice_content">
<div class="exe-text"><p style="text-align: justify;"><strong>1. Unidades de Procesamiento Neural (NPUs)</strong><br />Las NPUs (Neural Processing Units) son componentes especializados dentro de los procesadores que están diseñados para acelerar las operaciones de redes neuronales profundas (Deep Learning) y otros algoritmos de IA. Estos chips están optimizados para realizar cálculos que requieren una gran cantidad de operaciones matemáticas paralelas, como la multiplicación de matrices y la acumulación de resultados.</p>
<p style="text-align: justify;">Las NPUs son comunes en dispositivos como teléfonos inteligentes, automóviles autónomos y dispositivos IoT avanzados. Están optimizadas para tareas de inferencia de IA, es decir, ejecutar modelos entrenados de IA en tiempo real, como el reconocimiento de imágenes y la visión por computadora, donde se necesita un procesamiento de IA en tiempo real con bajo consumo de energía. </p>
<p style="text-align: justify;"><strong>Ejemplos de NPUs:</strong></p>
<ul style="text-align: justify;">
<li><strong>Apple Neural Engine:</strong> Integrado en los chips A-series y M-series de Apple, como el A14 o M2, este componente permite el procesamiento rápido de tareas de IA, como el reconocimiento facial, la fotografía computacional y el procesamiento de lenguaje natural.</li>
<li><strong>Huawei Kirin NPU:</strong> En los procesadores Kirin de Huawei, la NPU permite un alto rendimiento en tareas como la mejora de imágenes y la traducción en tiempo real.</li>
</ul>
<p style="text-align: justify;">Entre las ventajas que podemos encontrar en este tipo de procesadores están:</p>
<ul style="text-align: justify;">
<li><strong>Eficiencia energética:</strong> Las NPUs están diseñadas para ser eficientes en términos de consumo de energía, lo que las hace ideales para dispositivos portátiles.<br /><strong></strong></li>
<li><strong>Integración:</strong> Muchas veces están integradas dentro de los chips de smartphones (como los de Qualcomm o Huawei) para realizar tareas de IA sin necesidad de conectarse a un servidor.</li>
</ul>
<p style="text-align: justify;"><strong>2. Tensor Processing Units (TPUs)</strong><br />Las TPUs son procesadores desarrollados por Google específicamente para tareas de aprendizaje automático (Machine Learning) y aprendizaje profundo (Deep Learning). Aunque las TPUs son más comunes en centros de datos, Google también ha integrado versiones optimizadas de TPUs en dispositivos móviles, como los teléfonos Google Pixel.</p>
<p style="text-align: justify;">Las TPUs están optimizadas para realizar los cálculos que las redes neuronales necesitan, particularmente los modelos de aprendizaje profundo basados en tensores, lo que les permite realizar tareas de IA a velocidades mucho mayores que las CPU o incluso las GPU tradicionales.</p>
<p style="text-align: justify;">Específicamente, se diseñaron para acelerar las operaciones relacionadas con el procesamiento de tensores (matrices de datos multidimensionales) que se utilizan en las redes neuronales profundas. Están optimizadas tanto para la inferencia como para el entrenamiento de modelos de IA.</p>
<p style="text-align: justify;">Las TPUs están diseñadas para realizar cálculos masivos de matrices, que son esenciales en operaciones de aprendizaje profundo, como las multiplicaciones de matrices de grandes dimensiones. Son extremadamente eficientes para realizar operaciones de álgebra lineal que dominan las redes neuronales, en particular las redes neuronales convolucionales y las redes neuronales recurrentes.</p>
<p style="text-align: justify;">A diferencia de las NPUs, las TPUs están más enfocadas en el procesamiento en la <strong>nube y en centros de datos</strong>, ya que requieren gran capacidad de cómputo. Están diseñadas para cargas de trabajo de IA a gran escala, como el entrenamiento de modelos de IA en grandes bases de datos o la inferencia de modelos complejos en tiempo real.</p>
<p><strong>Ejemplos:</strong></p>
<ul>
<li><strong>TPU v4 de Google</strong>, utilizada en sus centros de datos para tareas de entrenamiento de IA masivas.</li>
<li><strong>Servicios de Google Cloud TPU</strong>, que permiten el acceso remoto a la potencia de TPUs a través de la nube.</li>
</ul>
<p style="text-align: justify;">En cuanto a sus ventajas, podemos destacar que:</p>
<ul>
<li><strong>Altamente escalables:</strong> Google ofrece TPUs en su servicio de Google Cloud para que los desarrolladores de IA puedan usarlas a gran escala en tareas de entrenamiento masivo.</li>
<li><strong>Optimización para TensorFlow:</strong> Están optimizadas para el framework de aprendizaje automático TensorFlow, lo que las hace muy eficientes en tareas de IA dentro del ecosistema de Google.</li>
</ul>
<p style="text-align: justify;"><strong>3. Unidades de Procesamiento Gráfico (GPUs)</strong><br />Las GPUs (Graphic Processing Units) son tradicionalmente conocidas por manejar el procesamiento de gráficos, pero se han vuelto fundamentales para el entrenamiento de modelos de IA debido a su capacidad de realizar cálculos masivamente paralelos. Las GPU pueden manejar los enormes volúmenes de datos y las operaciones complejas requeridas en el entrenamiento de redes neuronales.</p>
<p style="text-align: justify;"><strong>NVIDIA</strong>, un líder en el desarrollo de GPUs, ha adaptado sus unidades de procesamiento gráfico para tareas de IA, lanzando productos como <strong>CUDA</strong> y <strong>Tensor Cores</strong> que están específicamente diseñados para ejecutar operaciones de IA de manera eficiente.</p>
<p style="text-align: justify;"><strong>4. Arquitectura de Procesadores ARM y AI</strong><br />Los procesadores basados en la arquitectura ARM están cada vez más diseñados para soportar cargas de trabajo de IA. ARM ha desarrollado tecnologías como el <strong>Cortex-A y el Cortex-M</strong> con soporte integrado para acelerar tareas de IA tanto en dispositivos móviles como en sistemas embebidos.</p>
<p style="text-align: justify;"><strong>ARM</strong> también está avanzando con el <strong>Proyecto Trillium</strong>, que incluye la NPU <strong>ARM Ethos</strong>, diseñada específicamente para aplicaciones de <strong>aprendizaje automático</strong>, ofreciendo alto rendimiento en la inferencia de modelos de IA.</p>
<p style="text-align: justify;"><img src="1693834452953.jpeg" width="1024" height="576" alt="GPU vs CPU vs TPU" /></p></div>
</div>
</div>
</article>
<div id="packageLicense" class="cc cc-by-sa">
<p><span>Obra publicada con</span> <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Reconocimiento Compartir igual 4.0</a></p>
</div>
</section>
</div>
<div id='bottomPagination'>
<nav class="pagination noprt">
<a href="15_inteligencia_artificial_en_los_procesadores.html" class="prev"><span><span>&laquo; </span>Anterior</span></a> <span class="sep">| </span><a href="1511_diferencias_clave_entre_npu_y_tpu.html" class="next"><span>Siguiente<span> &raquo;</span></span></a>
</nav>
</div>
</div>
<p id="made-with-eXe"><a href="https://exelearning.net/" target="_blank" rel="noopener"><span>Creado con eXeLearning<span> (Ventana nueva)</span></span></a></p><script type="text/javascript" src="_adultos_js.js"></script></body></html>